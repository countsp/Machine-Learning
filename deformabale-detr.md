# Deformable DETR


![deformable-detr](https://github.com/fundamentalvision/Deformable-DETR/raw/main/figs/illustration.png)

**SenseTime与中科大、香港中文大学的研究人员合作发表在ICLR 2021上的论文，提出了一种改进版的DETR——Deformable DETR。该方法保留了DETR“端到端无需手工设计”的优势，同时解决了其训练收敛慢和小目标检测性能差的问题。**

![Screenshot from 2025-05-15 19-06-43](https://github.com/user-attachments/assets/e37860d6-1978-4083-9b55-6b386f8dc47c)

**论文核心是引入了Deformable Attention（可变形注意力模块）：**

* 该模块只关注少量的采样点（如每个查询4个点），而不是整个特征图；

* 采样点的偏移（offsets）和注意力权重（attention weights）都由查询特征生成；

* 支持多尺度特征输入，替代传统的FPN结构实现尺度融合；

* 注意力计算复杂度由 $O(H^2W^2)$ 降为线性 $O(HW)$。

**eg**

特征图尺寸：[C, H, W] = [512, 32, 32]

| 项目             | 标准 Transformer                 | Deformable Attention                          |
|------------------|----------------------------------|-----------------------------------------------|
| 输入特征         | `[1024, 512]`                    | 同左                                          |
| Q 维度           | `[1024, 8, 64]`                  | 同左（由 FC 得到）                            |
| K / V 维度       | `[1024, 8, 64]`                  | `[1024, 8, 4, 64]`（从 4 个采样点中得）       |
| 采样机制         | 全部像素做注意力（密集）         | 每个 Query 只采样少数点（如 4 个）            |
| 计算复杂度       | `O(N^2) = 10^6`                  | `O(N × 32)`（显著降低，近似线性）             |

**流程**
```
z_q ∈ [512]
   │
   ├─→ Linear(512→2) → 参考点 p_q ∈ [2]
   └─→ Linear(512→96) → reshape →
         ├─ Δp ∈ [8, 4, 2]
         └─ A ∈ [8, 4]  ← ✅ 就是 attention 权重

采样点位置 = p_q + Δp ∈ [8, 4, 2]
   ↓
双线性插值特征图 x ∈ [256, H, W] → [8, 4, 256]
   ↓
每个 head 用自己的 Linear 映射 Value → [8, 4, 64]
   ↓
使用 A ∈ [8, 4] 对 V 加权求和 → [8, 64]
   ↓
拼接 → 得到输出 ∈ [512]

```

**query 向量** z_q ∈ [512]
```
z_q = torch.linspace(0.0, 1.0, steps=512)
```

**得到参考点** p_q ∈ [2]
```
nn.Linear(512, 2) + sigmoid
```

得到一个参考比例： **x_q** = (0.4, 0.5)


**得到偏移点** Δp ∈ [8, 4, 2]

使用另一个大的 FC 层（如 nn.Linear(512, 3×M×K×L)）来预测：

偏移 Δp（offsets）：[8, K, L, 2]

权重 A（attention weights）：[8, K, L]


M = 8（head 数）

K = 4（一个head偏移K（4）次，得到4个）

L = 1（特征图层数，Encoder 中一般设为 1）

```
a = torch.tensor([
    a11, a12,  # Δp₁   ,   a13, a14,  # Δp₂
    a21, a22,  # Δp₃   ,   a23, a24,  # Δp₄
    a31, a32, a33, a34  # A₁~A₄
])
```


```
p sampled = x_p +Δp m      ,k ∈[8,4,2]
```

**得到像素坐标** [8,4,2]

p sampled 再乘以特征图大小（如 32 × 32）得到像素坐标：

```
img_coords = p_sampled * torch.tensor([W, H])  # shape: [8, 4, 2]
```


```
features ∈ [8, 4, 256]   ← 每个 head 的每个点一个 256 维向量
```


**映射得到 Key 和 Value** [8, 4, 64]

每个 head 用自己的 W_v ∈ [64×256] 映射
 
```
K ∈ [8, 4, 64]   ← 每个 head，4 个采样点，每点 Key 向量
V ∈ [8, 4, 64]   ← Value 同理
```

**加权求和**

 A ∈ [8, 4]  * V  ∈ [8, 4, 64] = [8,64]

```
weighted_V = [
    0.2408 × [1.0, 2.0, 3.0, 4.0...]    = [0.2408, 0.4816, 0.7224, 0.9632...]
    0.2663 × [5.0, 6.0, 7.0, 8.0...]    = [1.3315, 1.5978, 1.8641, 2.1304...]
    0.2170 × [9.0, 10.0, 11.0, 12.0...] = [1.9530, 2.1700, 2.3870, 2.6040...]
    0.2759 × [13.0, 14.0, 15.0, 16.0...]= [3.5867, 3.8626, 4.1385, 4.4144...]
]


列求和：
output = [
    0.2408 + 1.3315 + 1.9530 + 3.5867... = 7.1119,
    0.4816 + 1.5978 + 2.1700 + 3.8626... = 8.1120,
    0.7224 + 1.8641 + 2.3870 + 4.1385... = 9.1120,
    0.9632 + 2.1304 + 2.6040 + 4.4144... = 10.1120
]
```


**拼接所有 head** 

输出 [512] 



```
delta_p:
tensor([[0.1000, 0.2000],
        [0.3000, 0.4000],
        [0.5000, 0.6000],
        [0.7000, 0.8000]])

attn_weight:
tensor([0.2408, 0.2663, 0.2170, 0.2759])  # softmax 后
```

```
# 假设 V 特征维度为 256
V = torch.tensor([
    [1.0, 2.0, 3.0, 4.0...],
    [5.0, 6.0, 7.0, 8.0...],
    [9.0, 10.0, 11.0, 12.0..],
    [13.0, 14.0, 15.0, 16.0...]
])  # shape: [4, 256]

weighted_V = [
    0.2408 × [1.0, 2.0, 3.0, 4.0...]    = [0.2408, 0.4816, 0.7224, 0.9632...]
    0.2663 × [5.0, 6.0, 7.0, 8.0...]    = [1.3315, 1.5978, 1.8641, 2.1304...]
    0.2170 × [9.0, 10.0, 11.0, 12.0...] = [1.9530, 2.1700, 2.3870, 2.6040...]
    0.2759 × [13.0, 14.0, 15.0, 16.0...]= [3.5867, 3.8626, 4.1385, 4.4144...]
]


列求和：
output = [
    0.2408 + 1.3315 + 1.9530 + 3.5867... = 7.1119+...,
    0.4816 + 1.5978 + 2.1700 + 3.8626... = 8.1120+...,
    0.7224 + 1.8641 + 2.3870 + 4.1385... = 9.1120+...,
    0.9632 + 2.1304 + 2.6040 + 4.4144... = 10.1120+...
]

output = tensor([7.1119, 8.1120, 9.1120, 10.1120...])  # shape: [256]
```
