# Transfuser

![Screenshot from 2025-06-13 16-14-23](/home/office2004/Pictures/Screenshot from 2025-06-13 16-14-23.png)

这篇文章《TransFuser: Imitation with Transformer-Based Sensor Fusion for Autonomous Driving》主要围绕如何在端到端驾驶框架中，将相机图像与 LiDAR 点云两种互补传感器的信息，通过自注意力机制有效融合，从而显著提升复杂城市场景下的驾驶性能展开。下面分几个方面介绍其核心内容：

1. **研究动机与问题定义**
   - 当前大多数端到端驾驶（Imitation Learning, IL）方法在复杂、动态密集的城市场景中，单纯基于几何投影的多模态融合（如 Bird’s Eye View 与透视图之间的局部卷积操作）表现欠佳，难以捕捉远近交通灯、来车等跨模态的全局关联。
   - 本文提出在 IL 的框架下，直接将图像与 LiDAR 的中间特征视作一系列“token”，利用 Transformer 的自注意力（self-attention）来学习它们之间的全局依赖，从而改进多模态感知与决策能力。
2. **TransFuser 架构概览**
   - **多分辨率融合 Transformer**：在图像分支（ResNet 类似结构）和 BEV 分支（LiDAR 直方图表示）各自的多个中间层，插入 Transformer 模块，将两路特征拼接并添加位置编码后，通过多头自注意力融合，再分别加回各自分支。
   - **航路点（waypoint）自回归预测**：融合后得到的全局特征向量（512 维）经 MLP 降维为 64 维，初始化 GRU 隐状态，迭代输出未来 4 个差分航路点，再由两个 PID 控制器生成具体的转向、油门与制动指令。
   - **辅助任务多任务学习**：在训练时，除主干的航路点 L1 回归损失外，还附加图像分支的深度预测、语义分割，以及 BEV 分支的高清地图（HD map）分割和车辆检测等多种监督信号，提升中间特征的可解释性与鲁棒性。
3. **新的评价基准与实验结果**
   - **Longest6 基准**：在 CARLA 0.9.10 中，从官方 76 条路线上每城镇选取最长的 6 条（平均 1.5 km），并在高交通密度、红绿灯、行人和多车道环境下测试，重点考察模型在密集动态场景的表现。
   - **性能对比**：与多种基线（晚期融合、几何投影融合、PointPainting-based LAV、纯视觉 Latent TransFuser、WOR 强化学习方法等）比较，TransFuser 在 Driving Score 上超出最佳几何融合 19.98 %、将碰撞数平均降低近 48 %，并在官方秘密路线上也取得当时最高的 Infraction Score 和 Competitive Driving Score。
4. **主要贡献**
   - 设计了更具挑战性的多城镇长路线路线上端到端驾驶评价基准，揭示了现有几何融合 IL 方法在密集交通下的局限；
   - 提出 TransFuser，多尺度多模态自注意力融合机制，将全局上下文和跨模态交互直接嵌入特征提取层；
   - 在 Longest6 基准及 CARLA 官方排行榜上均实现了当时最优的端到端驾驶性能，同时通过消融分析展现了各模块（Transformer 融合、辅助任务、安全启发式）的作用与局限。

总之，TransFuser 将 Transformer 引入自动驾驶的感知与决策中，首次在端到端驾驶场景下证明了全局自注意力融合相较于传统局部几何方法的显著优势，对未来多模态自动驾驶系统的设计具有重要参考价值。





**输入图像和 LiDAR 表示**

- **图像输入**：使用三台相机（forward、左 60°、右 60°），每个相机采集分辨率 960×480 的图像，裁剪为 320×160 以去除畸变，再将三张无失真图拼接成一张 704×160 、水平视场 132° 的输入图像送入编码器。

- **LiDAR 输入**：将单帧点云投影到 32m×32m 的 BEV 网格（256×256 像素），高度维度离散为“地面以下/地面以上”两通道，并额外拼接当前目标点 (目标航点) 的 1 个热力图通道，共三通道伪图像。

  通道 0（“地面以下”）：收集所有 z<0的点，取其高度绝对值最大值（或二值占用）。用来表示坑洼、下陷区域或车下结构等信息。

  通道 1（“地面以上”）：收集所有 z≥0的点，取其最大 zi（或同样用二值占用）。
   这样得到 2 个通道的 “高度/占用” 信息。

  通道 2，在 下一个要到达的航路点（waypoint） 位置画一个小高斯点（或一圈圆斑），其余像素置 0，形成一个热力图。这样网络能直观地“看到”下一个目标在 BEV 上的位置。

**骨干网络（Backbone）**

- 两条分支都采用 RegNetY-3.2GF 骨干：
  - **图像分支**：加载在 ImageNet 上预训练的 RegNetY-3.2GF 。
  - **BEV 分支**：同样的 RegNetY-3.2GF 从头训练，用于编码 LiDAR BEV 伪图像。

**多尺度融合 Transformer**

- 在骨干网络的多个中间层（共 4 个尺度）插入 Transformer 模块：
  1. **特征提取**：假设某一尺度下，图像分支输出 Hs×Ws×C 的特征图，BEV 分支输出 H′s×W′s×C 的特征图。
  2. **Token 化**：将这两路特征图分别展平成 (Hs·Ws + H′s·W′s) 个 token，维度均为 C。
  3. **位置编码**：为每个 token 加入同维度的可学习位置编码，以保留空间信息。
  4. **自注意力融合**：将上述序列输入一个 Transformer 模块（每个尺度一个），利用 4 个 attention heads 计算跨模态的全局依赖关系，输出维度与输入一致。
  5. **重塑与残差相加**：将输出序列 reshape 回原始的 Hs×Ws×C 和 H′s×W′s×C ，分别与原特征图逐元素相加，实现深度融合。
- 这样，浅层和深层特征都能在图像与 LiDAR 之间进行全局信息交换，有效提升跨模态感知能力。

**融合后特征汇聚**

- 在最后一个尺度，各自分支得到 22×5×C （图像）和 8×8×C （BEV）的特征图，分别通过全局平均池化＋全连接层降到 512 维，再对两路 512 维向量做逐元素相加，得到一个紧凑的环境全局表示，随后送入 MLP＋GRU 进行航路点预测。



# lidar

**通道 0：地面以下高度**

- 对应所有点云中 z<0的点；
- 在 BEV 网格坐标 (u,v)处，取所有落在该格子且 z<0点的∣z∣ 最大值，赋给通道 0；
- 如果该格子内没有任何 z<0，通道 0 的值保持 0。

**通道 1：地面以上高度**

- 对应所有点云中 z≥0的点；
- 在同一格子 (u,v)处，取所有落在该格子且 z≥0点的 z最大值，赋给通道 1；
- 如果该格子内没有任何 z≥0的点，通道 1 的值保持 0。

**通道 2：下一个航路点热力图**

- 根据当前预测的下一个 waypoint (x∗,y∗)，在对应的 BEV 网格 (u∗,v∗)位置写一个高斯斑（或置 1），其余位置为 0。
